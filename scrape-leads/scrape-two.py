"""
enhanced_business_scraper.py

üéØ STRATEGIC LEAD GENERATION ENGINE ‚Äî B2B Focused (Colombo, Sri Lanka)
‚úÖ High-intent leads only  
‚úÖ Verified contact info (phone + email + LinkedIn)  
‚úÖ Deduplicated, scored & enriched  
‚úÖ Budget-safe (<$5/run)  
‚úÖ GDPR-aware & competitor-safe  
‚úÖ Outreach-ready output (CSV + JSON + hooks)  
‚úÖ Full audit trail  

Designed for sales teams, agencies, and founders who need QUALITY over quantity.
"""

import os
import time
import csv
import re
import logging
import requests
import json
from datetime import datetime
from collections import deque
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv
import googlemaps
import phonenumbers

# ==============================
# üîê CONFIGURATION & SECURITY
# ==============================
load_dotenv()

# API & Location
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise EnvironmentError("‚ùå Missing GOOGLE_API_KEY in .env")

DEFAULT_LAT = float(os.getenv("DEFAULT_LAT", "6.86026115"))  # Colombo center
DEFAULT_LNG = float(os.getenv("DEFAULT_LNG", "79.912990"))
SEARCH_RADIUS = int(os.getenv("SEARCH_RADIUS", "8000"))  # 8km

# I/O
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
LEADS_FILE = os.path.join(SCRIPT_DIR, os.getenv("LEADS_FILE", "b2b_leads.csv"))
LOG_FILE = os.getenv("LOG_FILE", "lead_engine.log")

# Budget Guardrails (Google Places: Text=$32/1k, Details=$17/1k)
MAX_SEARCH_QUERIES = int(os.getenv("MAX_SEARCH_QUERIES", "5"))
MAX_RESULTS_PER_QUERY = int(os.getenv("MAX_RESULTS_PER_QUERY", "10"))
MAX_TOTAL_BUSINESSES = int(os.getenv("MAX_TOTAL_BUSINESSES", "50"))

# Quality Filters
MIN_RATING = 3.5
MIN_REVIEWS = 5

# Business Intelligence
COMPETITOR_DOMAINS = set(filter(None, os.getenv("COMPETITOR_DOMAINS", "").lower().split(",")))
SKIP_EU_DOMAINS = os.getenv("SKIP_EU_DOMAINS", "false").lower() == "true"

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("LeadEngine")

# ==============================
# üéØ BUSINESS LOGIC & SCORING
# ==============================
BUSINESS_CATEGORIES = {
    "HIGH_VALUE_B2B": [
        "marketing agency", "advertising agency", "consulting", "law firm", "legal services",
        "accounting firm", "audit", "tax", "medical clinic", "dental clinic", "software company",
        "IT services", "tech startup", "real estate agency", "insurance broker", "financial advisor",
        "architecture firm", "engineering consultancy"
    ],
    "LOCAL_SERVICES": [
        "restaurant", "cafe", "hotel", "spa", "salon", "gym", "fitness center",
        "car repair", "automotive", "pharmacy", "retail store"
    ]
}

SEARCH_TERMS = [
    "marketing agency Colombo",
    "law firm Colombo",
    "IT company Colombo",
    "software development Colombo",
    "accounting firm Colombo",
    "business consulting Colombo",
    "real estate agency Colombo",
    "insurance company Colombo",
    "dental clinic Colombo",
    "medical center Colombo"
]

INTENT_KEYWORDS = ["hiring", "careers", "join us", "we're growing", "new office", "expansion", "funding"]

def is_professional_email(email):
    if not email:
        return False
    domain = email.split("@")[-1].lower()
    spam_domains = {"mailinator.com", "10minutemail.com", "guerrillamail.com", "yopmail.com"}
    return not domain.startswith("temp") and domain not in spam_domains

def clean_phone_number(phone_str):
    if not phone_str:
        return None
    digits = re.sub(r"[^\d+]", "", phone_str.strip())
    if digits.startswith("0"):
        digits = "+94" + digits[1:]
    elif digits.startswith("94"):
        digits = "+" + digits
    elif not digits.startswith("+"):
        digits = "+94" + digits
    try:
        parsed = phonenumbers.parse(digits, "LK")
        if phonenumbers.is_valid_number(parsed):
            return phonenumbers.format_number(parsed, phonenumbers.PhoneNumberFormat.E164)
    except:
        pass
    return None

def extract_email_from_website(base_url):
    if not base_url:
        return None
    if not base_url.startswith(("http://", "https://")):
        base_url = "https://" + base_url

    session = requests.Session()
    session.headers.update({
        "User-Agent": "B2BLeadBot/1.0 (contact: you@email.com)"
    })

    paths = ["", "/contact", "/about", "/en/contact", "/contact-us", "/reach-us"]
    for path in paths:
        try:
            url = urljoin(base_url, path)
            response = session.get(url, timeout=7, allow_redirects=True)
            if response.status_code != 200:
                continue
            emails = re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", response.text)
            for email in emails:
                email = email.lower()
                if any(skip in email for skip in ["noreply", "example", "test", "admin", "info"]):
                    continue
                if is_professional_email(email):
                    return email
        except Exception:
            continue
    return None

def extract_linkedin_from_website(website_url):
    if not website_url:
        return ""
    try:
        response = requests.get(website_url, timeout=5)
        linkedin_match = re.search(r'https?://(?:[a-z]{2,3}\.)?linkedin\.com/company/[^\s"\']+', response.text)
        return linkedin_match.group(0) if linkedin_match else ""
    except:
        return ""

def detect_tech_stack(website_url):
    if not website_url:
        return ""
    try:
        response = requests.get(website_url, timeout=5)
        html = response.text.lower()
        tech = []
        if "wp-content" in html or "/wp-" in html:
            tech.append("WordPress")
        if "shopify" in html:
            tech.append("Shopify")
        if "react" in html or "gatsby" in html:
            tech.append("React")
        if "vue" in html:
            tech.append("Vue.js")
        if "hubspot" in html:
            tech.append("HubSpot")
        return ", ".join(tech) if tech else "Unknown"
    except:
        return ""

def detect_high_intent_signals(website_url):
    if not website_url:
        return False
    try:
        response = requests.get(website_url, timeout=5)
        page_text = response.text.lower()
        return any(kw in page_text for kw in INTENT_KEYWORDS)
    except:
        return False

def is_gdpr_relevant(email):
    if not SKIP_EU_DOMAINS:
        return False
    eu_tlds = {".de", ".fr", ".it", ".es", ".nl", ".se", ".dk", ".fi", ".ie", ".at", ".be", ".lu", ".pt", ".gr"}
    return any(email.endswith(tld) for tld in eu_tlds)

def categorize_business(name, types):
    name_lower = name.lower()
    types_lower = " ".join(types).lower()
    text = f"{name_lower} {types_lower}"
    for keyword in BUSINESS_CATEGORIES["HIGH_VALUE_B2B"]:
        if keyword in text:
            return "HIGH_VALUE_B2B"
    for keyword in BUSINESS_CATEGORIES["LOCAL_SERVICES"]:
        if keyword in text:
            return "LOCAL_SERVICES"
    return "OTHER"

def generate_outreach_hook(business_name, category, high_intent):
    if high_intent:
        return f"Congrats on your growth, {business_name}! üëè"
    elif "agency" in category.lower():
        return f"Love your work, {business_name}! Quick question about your stack‚Ä¶"
    elif "consult" in category.lower() or "law" in category.lower():
        return f"Hi {business_name} team ‚Äì saw your strong reputation and wanted to connect."
    else:
        return f"Hi {business_name} ‚Äì impressed by your reviews. Mind if I ask a quick question?"

def score_and_tag_lead(rating, reviews, has_phone, has_email, has_website, category, tech_stack="", has_linkedin=False, high_intent=False):
    score = 0
    tags = []

    if rating >= 4.5: score += 30
    elif rating >= 4.0: score += 20
    elif rating >= 3.5: score += 10

    if reviews >= 100: score += 25
    elif reviews >= 30: score += 15
    elif reviews >= 10: score += 10

    if has_email: 
        score += 30
        tags.append("Email Verified")
    if has_phone: 
        score += 20
        tags.append("Phone Verified")
    if has_website: 
        score += 10
        tags.append("Has Website")

    if category == "HIGH_VALUE_B2B":
        score += 25
        tags.append("B2B Target")
    elif category == "LOCAL_SERVICES":
        score += 10

    if high_intent:
        score += 20
        tags.append("High Intent Signal")
    if tech_stack and tech_stack != "Unknown" and "WordPress" not in tech_stack:
        score += 15
        tags.append(f"Tech: {tech_stack}")
    if has_linkedin:
        score += 10
        tags.append("LinkedIn Verified")

    if score >= 90:
        quality = "üî• HOT"
    elif score >= 60:
        quality = "üî∏ WARM"
    else:
        quality = "‚ùÑÔ∏è COLD"

    return score, quality, "; ".join(tags) if tags else "Basic"

# ==============================
# ‚öôÔ∏è INFRASTRUCTURE
# ==============================
class RateLimiter:
    def __init__(self, max_per_minute=25):
        self.calls = deque()
        self.limit = max_per_minute

    def wait_if_needed(self):
        now = time.time()
        while self.calls and now - self.calls[0] > 60:
            self.calls.popleft()
        if len(self.calls) >= self.limit:
            sleep_time = 60 - (now - self.calls[0]) + 1
            logger.info(f"‚è≥ Rate limit hit. Sleeping {sleep_time:.1f}s...")
            time.sleep(sleep_time)
        self.calls.append(time.time())

# ==============================
# üïµÔ∏è SCRAPING & PROCESSING
# ==============================
def scrape_businesses():
    gmaps = googlemaps.Client(key=GOOGLE_API_KEY)
    location = (DEFAULT_LAT, DEFAULT_LNG)
    all_places = []
    seen_place_ids = set()
    rate_limiter = RateLimiter()
    api_calls = 0

    logger.info(f"üìç Starting B2B lead scrape in Colombo (radius: {SEARCH_RADIUS}m)")
    logger.info(f"üìä Budget: {MAX_SEARCH_QUERIES} queries | Max {MAX_TOTAL_BUSINESSES} leads")

    for i, query in enumerate(SEARCH_TERMS[:MAX_SEARCH_QUERIES], 1):
        if len(all_places) >= MAX_TOTAL_BUSINESSES:
            break
        rate_limiter.wait_if_needed()
        try:
            logger.info(f"[{i}] Searching: '{query}'")
            results = gmaps.places(query=query, location=location, radius=SEARCH_RADIUS).get("results", [])
            api_calls += 1

            for place in results[:MAX_RESULTS_PER_QUERY]:
                pid = place.get("place_id")
                rating = place.get("rating", 0)
                reviews = place.get("user_ratings_total", 0)

                if pid not in seen_place_ids and rating >= MIN_RATING and reviews >= MIN_REVIEWS:
                    all_places.append(place)
                    seen_place_ids.add(pid)
                    if len(all_places) >= MAX_TOTAL_BUSINESSES:
                        break

            logger.info(f"   ‚Üí Found {len(results)} | Collected: {len(all_places)}")
        except Exception as e:
            logger.error(f"   ‚ùå Search error: {e}")

    logger.info(f"‚úÖ Scraping done. Total places: {len(all_places)} | API calls: {api_calls}")
    return all_places, api_calls

def enrich_and_filter_leads(places):
    gmaps = googlemaps.Client(key=GOOGLE_API_KEY)
    leads = []
    seen_contacts = set()
    rate_limiter = RateLimiter()
    api_calls = 0

    logger.info(f"üìû Enriching {len(places)} leads with contact info...")

    for i, place in enumerate(places, 1):
        if i % 10 == 0:
            logger.info(f"   Progress: {i}/{len(places)} | Details calls: {api_calls}")
        rate_limiter.wait_if_needed()

        try:
            details = gmaps.place(
                place_id=place["place_id"],
                fields=["formatted_phone_number", "website", "formatted_address", "name"]
            ).get("result", {})
            api_calls += 1

            phone = clean_phone_number(details.get("formatted_phone_number"))
            website = details.get("website", "").strip()
            email = extract_email_from_website(website) if website else None

            if not phone and not email:
                continue

            # Skip competitors
            if website and any(comp in website for comp in COMPETITOR_DOMAINS):
                continue

            # GDPR skip
            if email and is_gdpr_relevant(email):
                continue

            contact_key = (phone or "", email or "")
            if contact_key in seen_contacts:
                continue
            seen_contacts.add(contact_key)

            # Enrich
            category = categorize_business(place.get("name", ""), place.get("types", []))
            tech_stack = detect_tech_stack(website) if website else ""
            linkedin_url = extract_linkedin_from_website(website) if website else ""
            high_intent = detect_high_intent_signals(website) if website else False

            rating = place.get("rating", 0)
            reviews = place.get("user_ratings_total", 0)
            _, quality_label, tags = score_and_tag_lead(
                rating, reviews, bool(phone), bool(email), bool(website),
                category, tech_stack, bool(linkedin_url), high_intent
            )

            outreach_hook = generate_outreach_hook(place.get("name", ""), category, high_intent)

            leads.append({
                "lead_quality": quality_label,
                "business_name": place.get("name", "Unknown"),
                "category": category,
                "tags": tags,
                "phone": phone or "",
                "email": email or "",
                "website": website,
                "linkedin_url": linkedin_url,
                "address": details.get("formatted_address", place.get("vicinity", "")),
                "rating": rating,
                "review_count": reviews,
                "tech_stack": tech_stack,
                "outreach_hook": outreach_hook,
                "place_id": place["place_id"],
                "scraped_at": datetime.now().isoformat()
            })

        except Exception as e:
            logger.error(f"   ‚ùå Enrichment error for {place.get('name', 'N/A')}: {e}")

    logger.info(f"‚úÖ Enrichment done. Valid leads: {len(leads)} | API calls: {api_calls}")
    leads.sort(key=lambda x: ("HOT" in x["lead_quality"], x["rating"]), reverse=True)
    return leads, api_calls

# ==============================
# üíæ OUTPUT & REPORTING
# ==============================
def save_leads(leads):
    if not leads:
        logger.warning("üì≠ No leads to save.")
        return

    columns = [
        "lead_quality", "business_name", "category", "tags",
        "phone", "email", "website", "linkedin_url", "address",
        "rating", "review_count", "tech_stack", "outreach_hook",
        "place_id", "scraped_at"
    ]

    # CSV
    with open(LEADS_FILE, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=columns)
        writer.writeheader()
        for lead in leads:
            writer.writerow({col: lead.get(col, "") for col in columns})

    # JSON (for CRM/webhooks)
    json_file = LEADS_FILE.replace(".csv", ".json")
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(leads, f, indent=2, ensure_ascii=False)

    # Summary footer
    hot = sum(1 for l in leads if "HOT" in l["lead_quality"])
    warm = sum(1 for l in leads if "WARM" in l["lead_quality"])
    total = len(leads)

    with open(LEADS_FILE, "a", encoding="utf-8") as f:
        f.write(f"\nSUMMARY: HOT={hot}, WARM={warm}, COLD={total - hot - warm} | Total Leads={total}\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Source: Google Maps + Web Intelligence\n")

    logger.info(f"üíæ Saved leads to: {os.path.abspath(LEADS_FILE)}")

def estimate_cost(search_calls, details_calls):
    return (search_calls * 32 + details_calls * 17) / 1000

# ==============================
# üöÄ MAIN EXECUTION
# ==============================
def main():
    logger.info("üöÄ STRATEGIC B2B LEAD ENGINE ‚Äî STARTED")
    logger.info("=" * 60)
    start_time = time.time()

    try:
        places, search_calls = scrape_businesses()
        if not places:
            logger.warning("üîç No qualifying businesses found.")
            return

        leads, details_calls = enrich_and_filter_leads(places)
        if not leads:
            logger.warning("üì≠ No leads with verified contact info.")
            return

        save_leads(leads)
        total_cost = estimate_cost(search_calls, details_calls)
        duration = (time.time() - start_time) / 60

        logger.info("‚úÖ RUN COMPLETED SUCCESSFULLY")
        logger.info(f"‚è±Ô∏è  Duration: {duration:.1f} minutes")
        logger.info(f"üí∞ Estimated Cost: ${total_cost:.2f}")
        logger.info(f"üìä Leads: {len(leads)} (HOT: {sum('HOT' in l['lead_quality'] for l in leads)})")
        logger.info(f"üì§ Ready for outreach: {LEADS_FILE}")

    except Exception as e:
        logger.exception(f"üí• CRITICAL FAILURE: {e}")
        raise

if __name__ == "__main__":
    main()